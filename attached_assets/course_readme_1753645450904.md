# 🏨 Real Estate AI Stack

This project is a locally-run AI-powered assistant for analyzing real estate course materials. It supports ingestion of multiple formats (PDF, DOCX, PPTX), indexes courses using LlamaIndex, and runs a Streamlit-based UI for querying course content using Mistral 7B (local GGUF).

---

## ✅ Features

- Multi-course document indexing with LlamaIndex
- Local LLM inference via `llama-cpp-python` using GGUF models
- Streamlit UI for querying indexed courses
- Automatic re-indexing, syllabus-weighted prioritization (if enabled)
- Support for PDFs, DOCXs, PPTXs

---

## 🧰 System Requirements

- **OS**: Windows 10/11 or Linux/macOS
- **Python**: 3.12 (installed via [Python.org](https://www.python.org/downloads/))
- **RAM**: 16GB+ recommended
- **GPU**: Optional, supports CPU-only mode

---

## 📦 Installation Instructions

### 1. Clone or unzip project folder

```bash
git clone https://github.com/YOUR_USERNAME/real_estate_ai_stack.git
cd real_estate_ai_stack
```

Or unzip `real_estate_ai_stack_final_flat.zip` to any folder.

---

### 2. Create a virtual environment (optional but recommended)

```bash
python -m venv venv
venv\Scripts\activate   # On Windows
source venv/bin/activate  # On Linux/macOS
```

---

### 3. Install Python dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

⚠️ If `llama-cpp-python` fails to build:

- Install Visual Studio Build Tools (C++) from [https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)
- Or use prebuilt wheel (if available for your CPU)

---

## 🧠 Model Setup (Mistral 7B GGUF)

Place the model file here:

```
models/
└── mistral-7b-instruct-v0.1.Q4_K_M.gguf
```

If missing, run:

```bash
python update_mistral_model.py
```

(This will download the model from HuggingFace or use local cache.)

---

## 📁 File Structure

```
real_estate_ai_stack/
﹣
🔍 app_multi_course.py              # Main Streamlit entrypoint
🔍 start_app.py                     # Shortcut launcher
🔍 mistral_query_engine.py          # Local LLM config + service context
🔍 requirements.txt
🔍 update_mistral_model.py
🔍 manual_index_all_courses.py
🗂️ models/
├── mistral-7b-instruct-v0.1.Q4_K_M.gguf
🗂️ indexed_courses/
├── [course folders...]
🗂️ raw_docs/
└── [your PDFs, DOCXs, PPTXs]
🔍 README.md
```

---

## 🚀 Run the App

```bash
python start_app.py
```

Visit `http://localhost:8501` in your browser.

---

## 💠 Troubleshooting

### 🔄 Missing `manual_index_all_courses.py`

Copy from backup or download again from the zip.

### ❌ `ImportError` with `llama_index`

Use `llama-index==0.10.20`. Avoid 0.9.x or early 0.10.x.

```bash
pip install "llama-index==0.10.20"
```

### 🧱 `llama-cpp-python` build failed

Install with:

```bash
pip install llama-cpp-python --prefer-binary --force-reinstall
```

Or skip local model by using OpenAI API temporarily.

---

## 📓 Tips

- All course documents should go in `raw_docs/`
- Indexed data is cached in `indexed_courses/`
- For new courses: delete and re-index via `manual_index_all_courses.py`
- Update `.env` for custom API keys if using OpenAI

---

## 🔐 Optional: Secure Deployment

To protect localhost from public access:

- Use firewall to block external port 8501
- Or use reverse proxy (nginx) with basic auth

---

## 📦 Zip and Backup

To create a fully flattened backup:

```bash
python create_backup_zip.py  # Optional helper script
```

This includes all Python scripts, models, and local indexes.

---

## 🤝 Credits

- Built using [LlamaIndex](https://github.com/jerryjliu/llama_index)
- Local inference via [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- UI via [Streamlit](https://streamlit.io/)

