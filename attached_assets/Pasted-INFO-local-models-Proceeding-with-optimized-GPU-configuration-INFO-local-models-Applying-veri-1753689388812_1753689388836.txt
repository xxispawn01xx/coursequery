INFO:local_models:Proceeding with optimized GPU configuration...
INFO:local_models:üöÄ Applying verified HuggingFace forum RTX 3060 fixes...
INFO:local_models:üîç Testing RTX 3060 with forum-verified method...
INFO:local_models:‚úÖ RTX 3060 ready - 12.0GB VRAM (forum fixes applied)
INFO:local_models:‚úÖ HF_TOKEN found: hf_yqKQa...xxxf
INFO:local_models:Mistral check - Models: False, Pipeline: False, Other models clear: True
INFO:local_models:üöÄ RTX 3060 memory after cleanup: 12853MB used / 12287MB total (104.6%)
INFO:local_models:üöÄ Loading models on memtest-verified RTX 3060...
INFO:local_models:Loading Mistral 7B model with RTX 3060 compatibility fixes...
INFO:local_models:üîÑ Attempting Llama 2 7B Chat...
INFO:local_models:üîç Step 1: RTX 3060 Fix - Testing model on CPU first...
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [01:37<00:00, 48.61s/it]
INFO:local_models:‚úÖ CPU test successful - model integrity verified!
INFO:local_models:üìù Loading tokenizer...
INFO:local_models:üöÄ Step 3: Loading on RTX 3060 GPU with optimizations...
INFO:local_models:Using 4-bit quantization for RTX 3060 12GB efficiency
INFO:local_models:Applying HuggingFace forum RTX 3060 fix...
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 10.72it/s]
INFO:local_models:Moving model to GPU manually (safer than device_map)
ERROR:local_models:GPU loading failed for Llama 2 7B Chat: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 22.51 GiB is allocated by PyTorch, and 1.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
INFO:local_models:üîÑ Attempting Mistral 7B Instruct...
INFO:local_models:üîç Step 1: RTX 3060 Fix - Testing model on CPU first...
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [02:24<00:00, 72.37s/it]
INFO:local_models:‚úÖ CPU test successful - model integrity verified!
INFO:local_models:üìù Loading tokenizer...
INFO:local_models:üöÄ Step 3: Loading on RTX 3060 GPU with optimizations...
INFO:local_models:Using 4-bit quantization for RTX 3060 12GB efficiency
INFO:local_models:Applying HuggingFace forum RTX 3060 fix...
Loading checkpoint shards:   0%|                                                                 | 0/2 [00:00<?, ?it/s]
































































































































































































































































































































































































































































































































































































































































































































