INFO:local_models:Initialized LocalModelManager with device: cuda:0
INFO:local_models:‚è±Ô∏è Loading local models (type: mistral) - Start time: 23:49:05
INFO:memory_optimizer:Memory Status:
INFO:memory_optimizer:  RAM: 9.5/23.8GB (39.9%)
INFO:memory_optimizer:  Swap: 1.1/38.4GB (2.8%)
INFO:memory_optimizer:  Available: 14.3GB
INFO:local_models:GPU Memory: 12.0GB - RTX 3060 can handle 7B models
INFO:local_models:Mistral check - Models: False, Pipeline: False
INFO:local_models:Loading Mistral 7B model...
INFO:local_models:Found GGUF model: mistral-7b-instruct-v0.1.Q4_K_M.gguf
INFO:local_models:Using GGUF model: C:\Users\AliDesktop\Desktop\CoursQueryRemote\coursequery\models\gguf\mistral-7b-instruct-v0.1.Q4_K_M.gguf
INFO:local_models:GGUF detected but using HuggingFace model for compatibility
INFO:local_models:Attempting to load Llama 2 7B Chat... ‚¨áÔ∏è  First download (may take several minutes)
INFO:local_models:Using 4-bit quantization for RTX 3060 12GB efficiency
INFO:local_models:Loading tokenizer from cache: C:\Users\AliDesktop\Desktop\CoursQueryRemote\coursequery\models
INFO:local_models:Loading model from cache: C:\Users\AliDesktop\Desktop\CoursQueryRemote\coursequery\models
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:41<00:00, 20.90s/it]
INFO:local_models:Successfully loaded Llama 2 7B Chat
Device set to use cuda:0
INFO:local_models:Mistral 7B model loaded successfully
INFO:local_models:Loading embedding model...
INFO:local_models:Loading embedding model from cache: C:\Users\AliDesktop\Desktop\CoursQueryRemote\coursequery\models - ‚¨áÔ∏è  First download (may take several minutes)
C:\Users\AliDesktop\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py:204: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.
  warnings.warn(
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
INFO:local_models:Embedding model loaded on cuda
INFO:local_models:‚úÖ All models loaded successfully (mistral) - Total time: 44.73s
INFO:__main__:Model manager loaded successfully
WARNING:course_indexer:HuggingFaceLLM not available, using manual generation
INFO:__main__:Query engine initialized successfully
INFO:__main__:Models and query engine loaded successfully
INFO:query_engine:Processing query for course 'vcpe': value my business...
Loading llama_index.core.storage.kvstore.simple_kvstore from C:\Users\AliDesktop\Desktop\CoursQueryRemote\coursequery\indexed_courses\vcpe\index\docstore.json.
Loading llama_index.core.storage.kvstore.simple_kvstore from C:\Users\AliDesktop\Desktop\CoursQueryRemote\coursequery\indexed_courses\vcpe\index\index_store.json.
INFO:llama_index.core.indices.loading:Loading all indices.
INFO:course_indexer:Loaded index for course: vcpe
INFO:local_models:üîç Generating embeddings for 1 texts - 23:50:05
Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]C:\Program Files\Python312\Lib\site-packages\torch\nn\modules\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.66it/s]
INFO:local_models:üìä Embeddings generated in 0.27s | 1 texts | 3.7 texts/sec
INFO:local_models:ü§î Starting response generation - 23:50:05.316