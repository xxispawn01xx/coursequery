NFO:local_models:Initialized LocalModelManager with device: cuda:0
INFO:local_models:Loading local models (type: mistral)...
INFO:memory_optimizer:Memory Status:
INFO:memory_optimizer:  RAM: 9.1/23.8GB (38.3%)
INFO:memory_optimizer:  Swap: 1.1/38.4GB (2.9%)
INFO:memory_optimizer:  Available: 14.7GB
INFO:local_models:GPU Memory: 12.0GB - RTX 3060 can handle 7B models
INFO:local_models:Mistral check - Models: False, Pipeline: False
INFO:local_models:Loading Mistral 7B model...
INFO:local_models:Found GGUF model: mistral-7b-instruct-v0.1.Q4_K_M.gguf
INFO:local_models:Using GGUF model: C:\Users\AliDesktop\Desktop\CoursQueryRemote\coursequery\models\gguf\mistral-7b-instruct-v0.1.Q4_K_M.gguf
INFO:local_models:GGUF detected but using HuggingFace model for compatibility
INFO:local_models:Attempting to load Llama 2 7B Chat... ⬇️  First download (may take several minutes)
INFO:local_models:Using 4-bit quantization for RTX 3060 12GB efficiency
INFO:local_models:Loading tokenizer from cache: C:\Users\AliDesktop\Desktop\CoursQueryRemote\coursequery\models
INFO:local_models:Loading model from cache: C:\Users\AliDesktop\Desktop\CoursQueryRemote\coursequery\models
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:45<00:00, 22.82s/it]
INFO:local_models:Successfully loaded Llama 2 7B Chat
Device set to use cuda:0
INFO:local_models:Mistral 7B model loaded successfully
INFO:local_models:Loading embedding model...
INFO:local_models:Loading embedding model from cache: C:\Users\AliDesktop\Desktop\CoursQueryRemote\coursequery\models - ⬇️  First download (may take several minutes)
C:\Users\AliDesktop\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py:204: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.
  warnings.warn(
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
INFO:local_models:Embedding model loaded on cuda
INFO:local_models:All models loaded successfully (mistral)
INFO:__main__:Model manager loaded successfully
WARNING:course_indexer:HuggingFaceLLM not available, using manual generation
INFO:__main__:Query engine initialized successfully
INFO:__main__:Models and query engine loaded successfully