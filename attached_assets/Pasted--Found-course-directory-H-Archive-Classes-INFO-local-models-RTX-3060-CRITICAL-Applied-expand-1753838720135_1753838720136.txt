âœ… Found course directory: H:\Archive Classes
INFO:local_models:ğŸ”§ RTX 3060 CRITICAL: Applied expandable_segments:True before GPU access
INFO:local_models:ğŸ”§ RTX 3060 Fix: Set CUDA_LAUNCH_BLOCKING=1
INFO:local_models:ğŸ”§ RTX 3060 Fix: Set TORCH_USE_CUDA_DSA=1
INFO:local_models:ğŸ§¹ RTX 3060: Applied aggressive memory cleanup and allocator reset
INFO:local_models:Initialized LocalModelManager with device: cuda:0
ğŸ“ Using local H:\ course directory: H:\Archive Classes\coursequery\archived_courses
ğŸ’¡ Running in pure offline mode
INFO:hybrid_query_engine:âœ… Whisper available for local transcription
INFO:hybrid_query_engine:Available models: {'local_embeddings': True, 'local_llm': True, 'whisper': True, 'openai_api': False, 'perplexity_api': False}
ğŸ“ Using local H:\ course directory: H:\Archive Classes\coursequery\archived_courses
ğŸ’¡ Running in pure offline mode
ğŸ“ Using local H:\ course directory: H:\Archive Classes\coursequery\archived_courses
ğŸ’¡ Running in pure offline mode
INFO:local_models:â±ï¸ Loading local models (type: mistral) - Start time: 21:24:59
INFO:memory_optimizer:Memory Status:
INFO:memory_optimizer:  RAM: 11.9/23.8GB (50.1%)
INFO:memory_optimizer:  Swap: 0.9/24.0GB (3.8%)
INFO:memory_optimizer:  Available: 11.9GB
INFO:local_models:GPU Memory: 12.0GB - RTX 3060 can handle 7B models
INFO:local_models:ğŸ”¬ RTX 3060 Configuration - Hardware verified healthy via memtestcl
INFO:local_models:âœ… Memtestcl confirmed RTX 3060 memory is healthy (0 errors)
INFO:local_models:Proceeding with optimized GPU configuration...
INFO:local_models:ğŸš€ Applying verified HuggingFace forum RTX 3060 fixes...
INFO:local_models:ğŸ§¹ Performing aggressive RTX 3060 memory cleanup...
INFO:local_models:ğŸ” Testing RTX 3060 with forum-verified method...
INFO:local_models:âœ… RTX 3060 ready - 12.0GB VRAM (forum fixes + memory optimization applied)
INFO:local_models:âœ… HF_TOKEN found: hf_yqKQa...xxxf
INFO:local_models:Mistral check - Models: False, Pipeline: False, Other models clear: True
INFO:local_models:ğŸš€ RTX 3060 memory after cleanup: 3509MB used / 12287MB total (28.6%)
INFO:local_models:ğŸš€ Loading models on memtest-verified RTX 3060...
INFO:local_models:Loading Mistral 7B model with RTX 3060 compatibility fixes...
WARNING:local_models:ğŸ”’ Using smaller models due to RTX 3060 memory fragmentation and PyTorch CVE-2025-32434
INFO:local_models:ğŸ’¡ RTX 3060 Strategy: Load smaller models first to test memory allocator fixes
INFO:local_models:ğŸ”„ Attempting DistilGPT-2 (Small, Safe for RTX 3060)...
INFO:local_models:ğŸ” Step 1: Loading with safetensors (secure format)...
INFO:local_models:âœ… Safetensors loading successful - secure format verified!
INFO:local_models:ğŸ“ Loading tokenizer...
INFO:local_models:ğŸš€ Step 3: Loading on RTX 3060 GPU with optimizations...
INFO:local_models:Using 4-bit quantization for RTX 3060 12GB efficiency
INFO:local_models:Applying HuggingFace forum RTX 3060 fix...
INFO:local_models:Moving model to GPU manually (safer than device_map)
INFO:local_models:ğŸ”§ Creating text generation pipeline...
Device set to use cuda:0
INFO:local_models:âœ… Successfully loaded DistilGPT-2 (Small, Safe for RTX 3060) with RTX 3060 optimizations!
INFO:local_models:Loading embedding model...
INFO:local_models:Loading embedding model from cache: H:\Archive Classes\coursequery\models - â¬‡ï¸  First download (may take several minutes)
ERROR:local_models:Failed to load embedding model: cannot access local variable 'torch' where it is not associated with a value
WARNING:local_models:Continuing without embedding model - Q&A will be limited
WARNING:local_models:Embedding model failed to load - Q&A functionality will be limited
INFO:local_models:âœ… All models loaded successfully (mistral) - Total time: 1.17s
INFO:__main__:Model manager loaded successfully with mistral
ğŸ“ Using local H:\ course directory: H:\Archive Classes\coursequery\archived_courses
ğŸ’¡ Running in pure offline mode
ğŸ“ Using local H:\ course directory: H:\Archive Classes\coursequery\archived_courses
ğŸ’¡ Running in pure offline mode
WARNING:course_indexer:HuggingFaceLLM not available, using manual generation
INFO:__main__:Query engine initialized successfully
INFO:__main__:Models and query engine loaded successfully with mistral
ğŸ“ Using local H:\ course directory: H:\Archive Classes\coursequery\archived_courses
ğŸ’¡ Running in pure offline mode
ğŸ”’ FORCED to use local directories:
ğŸ“ Raw docs: archived_courses
ğŸ“Š Indexed: indexed_courses
ğŸ“ Using local H:\ course directory: H:\Archive Classes\coursequery\archived_courses
ğŸ’¡ Running in pure offline mode
ğŸ“ Using local H:\ course directory: H:\Archive Classes\coursequery\archived_courses
ğŸ’¡ Running in pure offline mode