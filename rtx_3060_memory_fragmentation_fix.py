#!/usr/bin/env python3
"""
RTX 3060 Memory Fragmentation Fix
=================================

This script applies proven fixes for RTX 3060 memory fragmentation issues
that cause impossible memory allocation reports (22GB on 12GB GPU).

Based on PyTorch community solutions:
- CUDA memory allocator configuration
- Memory pool management
- Proper cleanup sequences

Usage:
    python rtx_3060_memory_fragmentation_fix.py
"""

import os
import subprocess
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def apply_cuda_memory_fixes():
    """Apply CUDA memory allocator fixes for RTX 3060."""
    logger.info("üîß Applying RTX 3060 memory fragmentation fixes...")
    
    # Set CUDA environment variables for memory management
    cuda_fixes = {
        "PYTORCH_CUDA_ALLOC_CONF": "expandable_segments:True",
        "CUDA_LAUNCH_BLOCKING": "1",
        "TORCH_USE_CUDA_DSA": "1",
        "CUDA_VISIBLE_DEVICES": "0"
    }
    
    for key, value in cuda_fixes.items():
        os.environ[key] = value
        logger.info(f"‚úÖ Set {key}={value}")
    
    # Additional RTX 3060 specific optimizations
    logger.info("üöÄ Applying RTX 3060 optimizations...")
    
    try:
        import torch
        if torch.cuda.is_available():
            # Clear any existing fragmentation
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # Get actual GPU memory info
            device = torch.cuda.current_device()
            total_memory = torch.cuda.get_device_properties(device).total_memory
            allocated = torch.cuda.memory_allocated(device)
            
            logger.info(f"üéØ RTX 3060 Status:")
            logger.info(f"   Total Memory: {total_memory / 1024**3:.1f} GB")
            logger.info(f"   Allocated: {allocated / 1024**2:.0f} MB")
            logger.info(f"   Free: {(total_memory - allocated) / 1024**3:.1f} GB")
            
            # Apply memory pool settings
            if hasattr(torch.cuda, 'set_memory_fraction'):
                torch.cuda.set_memory_fraction(0.95)  # Use 95% max
                logger.info("‚úÖ Set memory fraction to 95%")
            
            return True
        else:
            logger.warning("‚ö†Ô∏è  CUDA not available - fixes applied for next restart")
            return False
            
    except ImportError:
        logger.info("PyTorch not imported yet - environment variables set")
        return False

def test_memory_allocation():
    """Test if memory allocation works correctly after fixes."""
    try:
        import torch
        if not torch.cuda.is_available():
            logger.info("CUDA not available for testing")
            return False
        
        logger.info("üß™ Testing memory allocation...")
        
        # Test small allocation
        test_tensor = torch.zeros(100, 100, device='cuda')
        logger.info("‚úÖ Small allocation successful")
        
        # Test larger allocation
        test_tensor2 = torch.zeros(1000, 1000, device='cuda')
        logger.info("‚úÖ Medium allocation successful")
        
        # Cleanup
        del test_tensor, test_tensor2
        torch.cuda.empty_cache()
        
        # Check final memory state
        allocated = torch.cuda.memory_allocated(0)
        total = torch.cuda.get_device_properties(0).total_memory
        usage_percent = (allocated / total) * 100
        
        logger.info(f"‚úÖ Memory test passed")
        logger.info(f"   Final usage: {allocated / 1024**2:.0f}MB ({usage_percent:.1f}%)")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Memory test failed: {e}")
        return False

def main():
    """Main function to apply all RTX 3060 fixes."""
    logger.info("üöÄ Starting RTX 3060 Memory Fragmentation Fix")
    
    # Apply CUDA fixes
    apply_cuda_memory_fixes()
    
    # Test if fixes work
    if test_memory_allocation():
        logger.info("‚úÖ RTX 3060 memory fragmentation fixes applied successfully!")
        logger.info("üí° Restart your application to ensure all fixes are active")
    else:
        logger.info("‚ö†Ô∏è  Fixes applied - restart required to test GPU functionality")
    
    logger.info("üìã Applied fixes:")
    logger.info("   - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
    logger.info("   - CUDA_LAUNCH_BLOCKING=1")
    logger.info("   - TORCH_USE_CUDA_DSA=1")
    logger.info("   - Memory fraction set to 95%")

if __name__ == "__main__":
    main()